{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8919c8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (50000, 32, 32, 3)\n",
      "Training labels shape:  (50000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "### CELL-2 ###\n",
    "# Load the raw CIFAR-10 data\n",
    "from data_utils import load_CIFAR10\n",
    "\n",
    "cifar10_dir = 'cifar-10-batches-py'\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "    del X_train, y_train\n",
    "    del X_test, y_test\n",
    "    print('Clear previously loaded data.')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print('Training data shape: ', X_train.shape)\n",
    "print('Training labels shape: ', y_train.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01912bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cb1f054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import utils, layers, activations, models, losses, optimizers, metrics, callbacks, datasets, preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e5e1cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79f669e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_label, test_data, test_label = load_CIFAR10(cifar10_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b34925a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing\n",
    "train_data_proc = train_data / 255\n",
    "test_data_proc = test_data / 255\n",
    "\n",
    "#One hot encoding\n",
    "train_label_proc = utils.to_categorical(train_label, 10)\n",
    "test_label_proc = utils.to_categorical(test_label, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6a5bcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen = preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=20,\n",
    "    shear_range=0.05,\n",
    "    zoom_range=0.3\n",
    ")\n",
    "train_gen = image_gen.flow(train_data_proc, train_label_proc, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0616807",
   "metadata": {},
   "outputs": [],
   "source": [
    "mInput = layers.Input((32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1c810de",
   "metadata": {},
   "outputs": [],
   "source": [
    "mB1 = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\", name=\"B1_Conv_1\")(mInput)\n",
    "mB1 = layers.BatchNormalization(name=\"B1_Norm_1\")(mB1)\n",
    "mB1 = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\", name=\"B1_Conv_2\")(mB1)\n",
    "mB1 = layers.BatchNormalization(name=\"B1_Norm_2\")(mB1)\n",
    "mB1 = layers.MaxPool2D(2, strides=1, name=\"B1_Pool\")(mB1)\n",
    "mB1 = layers.Dropout(0.2, name=\"B1_Drop\")(mB1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19288550",
   "metadata": {},
   "outputs": [],
   "source": [
    "mB2 = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\", name=\"B2_Conv_1\")(mB1)\n",
    "mB2 = layers.BatchNormalization(name=\"B2_Norm_1\")(mB2)\n",
    "mB2 = layers.Conv2D(128, 3, dilation_rate=2, activation=\"relu\", name=\"B2_Conv_2\")(mB2)\n",
    "mB2 = layers.BatchNormalization(name=\"B2_Norm_2\")(mB2)\n",
    "mB2 = layers.MaxPool2D(2, strides=1, name=\"B2_Pool\")(mB2)\n",
    "mB2 = layers.Dropout(0.25, name=\"B2_Drop\")(mB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "489295c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "B1_Conv_1 (Conv2D)              (None, 32, 32, 64)   1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B1_Norm_1 (BatchNormalization)  (None, 32, 32, 64)   256         B1_Conv_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B1_Conv_2 (Conv2D)              (None, 32, 32, 64)   36928       B1_Norm_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B1_Norm_2 (BatchNormalization)  (None, 32, 32, 64)   256         B1_Conv_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B1_Pool (MaxPooling2D)          (None, 31, 31, 64)   0           B1_Norm_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B1_Drop (Dropout)               (None, 31, 31, 64)   0           B1_Pool[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B3_Conv_1 (Conv2D)              (None, 31, 31, 128)  73856       B1_Drop[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B3_Norm_1 (BatchNormalization)  (None, 31, 31, 128)  512         B3_Conv_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B3_Conv_2 (Conv2D)              (None, 31, 31, 256)  295168      B3_Norm_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B3_Norm_2 (BatchNormalization)  (None, 31, 31, 256)  1024        B3_Conv_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B3_Pool (MaxPooling2D)          (None, 30, 30, 256)  0           B3_Norm_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B3_Drop (Dropout)               (None, 30, 30, 256)  0           B3_Pool[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B4_Conv_1 (Conv2D)              (None, 30, 30, 128)  295040      B3_Drop[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B4_Norm_1 (BatchNormalization)  (None, 30, 30, 128)  512         B4_Conv_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B4_Conv_2 (Conv2D)              (None, 30, 30, 256)  295168      B4_Norm_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B4_Add (Add)                    (None, 30, 30, 256)  0           B4_Conv_2[0][0]                  \n",
      "                                                                 B3_Pool[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B4_Norm_2 (BatchNormalization)  (None, 30, 30, 256)  1024        B4_Add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "B4_Drop (Dropout)               (None, 30, 30, 256)  0           B4_Norm_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B5_Conv_1 (Conv2D)              (None, 30, 30, 128)  295040      B4_Norm_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B6_Conv_1 (Conv2D)              (None, 30, 30, 128)  819328      B4_Norm_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B7_Conv_1 (Conv2D)              (None, 30, 30, 128)  295040      B4_Drop[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B5_Norm (BatchNormalization)    (None, 30, 30, 128)  512         B5_Conv_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B6_Norm (BatchNormalization)    (None, 30, 30, 128)  512         B6_Conv_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B7_Norm_1 (BatchNormalization)  (None, 30, 30, 128)  512         B7_Conv_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B5_Conv_2 (Conv2D)              (None, 30, 30, 256)  295168      B5_Norm[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B6_Conv_2 (Conv2D)              (None, 30, 30, 256)  819456      B6_Norm[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B7_Conv_2 (Conv2D)              (None, 30, 30, 256)  295168      B7_Norm_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B7_Add (Add)                    (None, 30, 30, 256)  0           B5_Conv_2[0][0]                  \n",
      "                                                                 B6_Conv_2[0][0]                  \n",
      "                                                                 B7_Conv_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B7_Norm_2 (BatchNormalization)  (None, 30, 30, 256)  1024        B7_Add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "B7_Drop (Dropout)               (None, 30, 30, 256)  0           B7_Norm_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B8_Conv_1 (Conv2D)              (None, 30, 30, 128)  295040      B7_Drop[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B8_Norm_1 (BatchNormalization)  (None, 30, 30, 128)  512         B8_Conv_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B8_Conv_2 (Conv2D)              (None, 30, 30, 256)  295168      B8_Norm_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B8_Add (Add)                    (None, 30, 30, 256)  0           B3_Pool[0][0]                    \n",
      "                                                                 B5_Conv_2[0][0]                  \n",
      "                                                                 B7_Drop[0][0]                    \n",
      "                                                                 B8_Conv_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B8_Norm_2 (BatchNormalization)  (None, 30, 30, 256)  1024        B8_Add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "B8_Drop (Dropout)               (None, 30, 30, 256)  0           B8_Norm_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B9_Conv_1 (Conv2D)              (None, 28, 28, 256)  590080      B8_Drop[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B9_Norm_1 (BatchNormalization)  (None, 28, 28, 256)  1024        B9_Conv_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B9_Conv_2 (Conv2D)              (None, 24, 24, 256)  590080      B9_Norm_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B9_Norm_2 (BatchNormalization)  (None, 24, 24, 256)  1024        B9_Conv_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B9_Pool (MaxPooling2D)          (None, 12, 12, 256)  0           B9_Norm_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "B9_Drop (Dropout)               (None, 12, 12, 256)  0           B9_Pool[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B8_Conv (Conv2D)                (None, 12, 12, 512)  1180160     B9_Drop[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B10_Pool (GlobalAveragePooling2 (None, 512)          0           B8_Conv[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "B10_Norm (BatchNormalization)   (None, 512)          2048        B10_Pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Output (Dense)                  (None, 10)           5130        B10_Norm[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,784,586\n",
      "Trainable params: 6,778,698\n",
      "Non-trainable params: 5,888\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mB3     = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\", name=\"B3_Conv_1\")(mB1)\n",
    "mB3     = layers.BatchNormalization(name=\"B3_Norm_1\")(mB3)\n",
    "mB3     = layers.Conv2D(256, 3, padding=\"same\", dilation_rate=2, activation=\"relu\", name=\"B3_Conv_2\")(mB3)\n",
    "mB3     = layers.BatchNormalization(name=\"B3_Norm_2\")(mB3)\n",
    "mB3Pool = layers.MaxPool2D(2, strides=1, name=\"B3_Pool\")(mB3)\n",
    "mB3     = layers.Dropout(0.3, name=\"B3_Drop\")(mB3Pool)\n",
    "\n",
    "mB4     = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\", name=\"B4_Conv_1\")(mB3)\n",
    "mB4     = layers.BatchNormalization(name=\"B4_Norm_1\")(mB4)\n",
    "mB4     = layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\", name=\"B4_Conv_2\")(mB4)\n",
    "mB4     = layers.add([mB4, mB3Pool], name=\"B4_Add\")\n",
    "mB4Nor  = layers.BatchNormalization(name=\"B4_Norm_2\")(mB4)\n",
    "mB4Dout = layers.Dropout(0.35, name=\"B4_Drop\")(mB4Nor)\n",
    "\n",
    "mB5 = layers.Conv2D(128, 3, dilation_rate=2, padding=\"same\", activation=\"relu\", name=\"B5_Conv_1\")(mB4Nor)\n",
    "mB5 = layers.BatchNormalization(name=\"B5_Norm\")(mB5)\n",
    "mB5 = layers.Conv2D(256, 3, dilation_rate=3, padding=\"same\", activation=\"relu\", name=\"B5_Conv_2\")(mB5)\n",
    "\n",
    "mB6 = layers.Conv2D(128, 5, padding=\"same\", activation=\"relu\", name=\"B6_Conv_1\")(mB4Nor)\n",
    "mB6 = layers.BatchNormalization(name=\"B6_Norm\")(mB6)\n",
    "mB6 = layers.Conv2D(256, 5, padding=\"same\", activation=\"relu\", name=\"B6_Conv_2\")(mB6)\n",
    "\n",
    "mB7 = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\", name=\"B7_Conv_1\")(mB4Dout)\n",
    "mB7 = layers.BatchNormalization(name=\"B7_Norm_1\")(mB7)\n",
    "mB7 = layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\", name=\"B7_Conv_2\")(mB7)\n",
    "mB7 = layers.add([mB5, mB6, mB7], name=\"B7_Add\")\n",
    "mB7 = layers.BatchNormalization(name=\"B7_Norm_2\")(mB7)\n",
    "mB7 = layers.Dropout(0.4, name=\"B7_Drop\")(mB7)\n",
    "\n",
    "mB8 = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\", name=\"B8_Conv_1\")(mB7)\n",
    "mB8 = layers.BatchNormalization(name=\"B8_Norm_1\")(mB8)\n",
    "mB8 = layers.Conv2D(256, 3, padding=\"same\", dilation_rate=2, activation=\"relu\", name=\"B8_Conv_2\")(mB8)\n",
    "mB8 = layers.add([mB3Pool, mB5, mB7, mB8], name=\"B8_Add\")\n",
    "mB8 = layers.BatchNormalization(name=\"B8_Norm_2\")(mB8)\n",
    "mB8 = layers.Dropout(0.45, name=\"B8_Drop\")(mB8)\n",
    "\n",
    "mB9 = layers.Conv2D(256, 3, activation=\"relu\", name=\"B9_Conv_1\")(mB8)\n",
    "mB9 = layers.BatchNormalization(name=\"B9_Norm_1\")(mB9)\n",
    "mB9 = layers.Conv2D(256, 3, dilation_rate=2, activation=\"relu\", name=\"B9_Conv_2\")(mB9)\n",
    "mB9 = layers.BatchNormalization(name=\"B9_Norm_2\")(mB9)\n",
    "mB9 = layers.MaxPool2D(2, name=\"B9_Pool\")(mB9)\n",
    "mB9 = layers.Dropout(0.5, name=\"B9_Drop\")(mB9)\n",
    "\n",
    "mB10 = layers.Conv2D(512, 3, padding=\"same\", activation=\"relu\", name=\"B8_Conv\")(mB9)\n",
    "mB10 = layers.GlobalAveragePooling2D(name=\"B10_Pool\")(mB10)\n",
    "mB10 = layers.BatchNormalization(name=\"B10_Norm\")(mB10)\n",
    "\n",
    "mOutput = layers.Dense(10, activation=\"softmax\", name=\"Output\")(mB10)\n",
    "\n",
    "model = models.Model(mInput, mOutput)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d08275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(lr=0.0001, decay=1e-6)\n",
    "\n",
    "model.compile(\n",
    "    loss=losses.CategoricalCrossentropy(),\n",
    "    optimizer=optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66c77943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1563 steps, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "1563/1563 [==============================] - 217s 139ms/step - loss: 1.7427 - accuracy: 0.3662 - val_loss: 4.3009 - val_accuracy: 0.2130\n",
      "Epoch 2/100\n",
      "1563/1563 [==============================] - 212s 136ms/step - loss: 1.3828 - accuracy: 0.5080 - val_loss: 1.4060 - val_accuracy: 0.5591\n",
      "Epoch 3/100\n",
      "1563/1563 [==============================] - 212s 136ms/step - loss: 1.1550 - accuracy: 0.5960 - val_loss: 1.3782 - val_accuracy: 0.5785\n",
      "Epoch 4/100\n",
      "1563/1563 [==============================] - 212s 136ms/step - loss: 0.9928 - accuracy: 0.6545 - val_loss: 0.8812 - val_accuracy: 0.7053\n",
      "Epoch 5/100\n",
      "1563/1563 [==============================] - 211s 135ms/step - loss: 0.8800 - accuracy: 0.6970 - val_loss: 1.0368 - val_accuracy: 0.6768\n",
      "Epoch 6/100\n",
      "1563/1563 [==============================] - 211s 135ms/step - loss: 0.7945 - accuracy: 0.7282 - val_loss: 0.6908 - val_accuracy: 0.7772\n",
      "Epoch 7/100\n",
      "1563/1563 [==============================] - 210s 135ms/step - loss: 0.7249 - accuracy: 0.7513 - val_loss: 0.6559 - val_accuracy: 0.7838\n",
      "Epoch 8/100\n",
      "1563/1563 [==============================] - 210s 135ms/step - loss: 0.6715 - accuracy: 0.7687 - val_loss: 0.6939 - val_accuracy: 0.7808\n",
      "Epoch 9/100\n",
      "1563/1563 [==============================] - 210s 135ms/step - loss: 0.6278 - accuracy: 0.7856 - val_loss: 0.5299 - val_accuracy: 0.8289\n",
      "Epoch 10/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.5917 - accuracy: 0.7997 - val_loss: 0.6152 - val_accuracy: 0.8134\n",
      "Epoch 11/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.5562 - accuracy: 0.8111 - val_loss: 0.5141 - val_accuracy: 0.8404\n",
      "Epoch 12/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.5281 - accuracy: 0.8187 - val_loss: 0.5395 - val_accuracy: 0.8347\n",
      "Epoch 13/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.5066 - accuracy: 0.8274 - val_loss: 0.4779 - val_accuracy: 0.8567\n",
      "Epoch 14/100\n",
      "1563/1563 [==============================] - 223s 142ms/step - loss: 0.4835 - accuracy: 0.8323 - val_loss: 0.5044 - val_accuracy: 0.8499\n",
      "Epoch 15/100\n",
      "1563/1563 [==============================] - 221s 141ms/step - loss: 0.4600 - accuracy: 0.8438 - val_loss: 0.5429 - val_accuracy: 0.8407\n",
      "Epoch 16/100\n",
      "1563/1563 [==============================] - 211s 135ms/step - loss: 0.4463 - accuracy: 0.8465 - val_loss: 0.4916 - val_accuracy: 0.8472\n",
      "Epoch 17/100\n",
      "1563/1563 [==============================] - 210s 135ms/step - loss: 0.4256 - accuracy: 0.8550 - val_loss: 0.4298 - val_accuracy: 0.8648\n",
      "Epoch 18/100\n",
      "1563/1563 [==============================] - 211s 135ms/step - loss: 0.4088 - accuracy: 0.8595 - val_loss: 0.4836 - val_accuracy: 0.8461\n",
      "Epoch 19/100\n",
      "1563/1563 [==============================] - 211s 135ms/step - loss: 0.3996 - accuracy: 0.8620 - val_loss: 0.4027 - val_accuracy: 0.8757\n",
      "Epoch 20/100\n",
      "1563/1563 [==============================] - 210s 135ms/step - loss: 0.3866 - accuracy: 0.8685 - val_loss: 0.4035 - val_accuracy: 0.8724\n",
      "Epoch 21/100\n",
      "1563/1563 [==============================] - 211s 135ms/step - loss: 0.3716 - accuracy: 0.8719 - val_loss: 0.3873 - val_accuracy: 0.8796\n",
      "Epoch 22/100\n",
      "1563/1563 [==============================] - 211s 135ms/step - loss: 0.3596 - accuracy: 0.8769 - val_loss: 0.4201 - val_accuracy: 0.8721\n",
      "Epoch 23/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.3516 - accuracy: 0.8798 - val_loss: 0.4156 - val_accuracy: 0.8765\n",
      "Epoch 24/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.3400 - accuracy: 0.8850 - val_loss: 0.4390 - val_accuracy: 0.8678\n",
      "Epoch 25/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.3326 - accuracy: 0.8842 - val_loss: 0.3809 - val_accuracy: 0.8867\n",
      "Epoch 26/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.3202 - accuracy: 0.8876 - val_loss: 0.3603 - val_accuracy: 0.8868\n",
      "Epoch 27/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.3154 - accuracy: 0.8912 - val_loss: 0.3594 - val_accuracy: 0.8929\n",
      "Epoch 28/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.3039 - accuracy: 0.8951 - val_loss: 0.3949 - val_accuracy: 0.8854\n",
      "Epoch 29/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2950 - accuracy: 0.8983 - val_loss: 0.3407 - val_accuracy: 0.8937\n",
      "Epoch 30/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.2893 - accuracy: 0.9011 - val_loss: 0.3659 - val_accuracy: 0.8946\n",
      "Epoch 31/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2823 - accuracy: 0.9022 - val_loss: 0.5059 - val_accuracy: 0.8646\n",
      "Epoch 32/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2803 - accuracy: 0.9028 - val_loss: 0.3700 - val_accuracy: 0.8964\n",
      "Epoch 33/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2694 - accuracy: 0.9072 - val_loss: 0.3983 - val_accuracy: 0.8870\n",
      "Epoch 34/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2573 - accuracy: 0.9109 - val_loss: 0.4000 - val_accuracy: 0.8898\n",
      "Epoch 35/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2566 - accuracy: 0.9110 - val_loss: 0.3358 - val_accuracy: 0.9015\n",
      "Epoch 36/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.2525 - accuracy: 0.9129 - val_loss: 0.4069 - val_accuracy: 0.8952\n",
      "Epoch 37/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2388 - accuracy: 0.9168 - val_loss: 0.3683 - val_accuracy: 0.8960\n",
      "Epoch 38/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2440 - accuracy: 0.9158 - val_loss: 0.4158 - val_accuracy: 0.8877\n",
      "Epoch 39/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2338 - accuracy: 0.9176 - val_loss: 0.4280 - val_accuracy: 0.8863\n",
      "Epoch 40/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.2308 - accuracy: 0.9206 - val_loss: 0.3553 - val_accuracy: 0.9015\n",
      "Epoch 41/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2284 - accuracy: 0.9213 - val_loss: 0.3526 - val_accuracy: 0.9029\n",
      "Epoch 42/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2224 - accuracy: 0.9227 - val_loss: 0.3867 - val_accuracy: 0.8963\n",
      "Epoch 43/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2152 - accuracy: 0.9248 - val_loss: 0.3663 - val_accuracy: 0.9056\n",
      "Epoch 44/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2140 - accuracy: 0.9262 - val_loss: 0.4028 - val_accuracy: 0.8900\n",
      "Epoch 45/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.2135 - accuracy: 0.9260 - val_loss: 0.3959 - val_accuracy: 0.8925\n",
      "Epoch 46/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2105 - accuracy: 0.9278 - val_loss: 0.3854 - val_accuracy: 0.8922\n",
      "Epoch 47/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2009 - accuracy: 0.9308 - val_loss: 0.3490 - val_accuracy: 0.9029\n",
      "Epoch 48/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1976 - accuracy: 0.9317 - val_loss: 0.3476 - val_accuracy: 0.9046\n",
      "Epoch 49/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1980 - accuracy: 0.9313 - val_loss: 0.3585 - val_accuracy: 0.9057\n",
      "Epoch 50/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1888 - accuracy: 0.9340 - val_loss: 0.3610 - val_accuracy: 0.9086\n",
      "Epoch 51/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1890 - accuracy: 0.9347 - val_loss: 0.3506 - val_accuracy: 0.9086\n",
      "Epoch 52/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1868 - accuracy: 0.9349 - val_loss: 0.4106 - val_accuracy: 0.8984\n",
      "Epoch 53/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1820 - accuracy: 0.9369 - val_loss: 0.3928 - val_accuracy: 0.8972\n",
      "Epoch 54/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1817 - accuracy: 0.9372 - val_loss: 0.3424 - val_accuracy: 0.9081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1758 - accuracy: 0.9394 - val_loss: 0.3411 - val_accuracy: 0.9113\n",
      "Epoch 56/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1784 - accuracy: 0.9381 - val_loss: 0.3958 - val_accuracy: 0.8995\n",
      "Epoch 57/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1719 - accuracy: 0.9408 - val_loss: 0.3828 - val_accuracy: 0.9049\n",
      "Epoch 58/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1708 - accuracy: 0.9409 - val_loss: 0.3530 - val_accuracy: 0.9093\n",
      "Epoch 59/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1641 - accuracy: 0.9437 - val_loss: 0.3317 - val_accuracy: 0.9133\n",
      "Epoch 60/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1661 - accuracy: 0.9438 - val_loss: 0.3376 - val_accuracy: 0.9151\n",
      "Epoch 61/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1611 - accuracy: 0.9451 - val_loss: 0.3320 - val_accuracy: 0.9093\n",
      "Epoch 62/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1594 - accuracy: 0.9441 - val_loss: 0.3478 - val_accuracy: 0.9080\n",
      "Epoch 63/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1565 - accuracy: 0.9465 - val_loss: 0.3659 - val_accuracy: 0.9088\n",
      "Epoch 64/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1542 - accuracy: 0.9476 - val_loss: 0.3289 - val_accuracy: 0.9161\n",
      "Epoch 65/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1547 - accuracy: 0.9470 - val_loss: 0.3482 - val_accuracy: 0.9121\n",
      "Epoch 66/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1484 - accuracy: 0.9479 - val_loss: 0.3767 - val_accuracy: 0.9091\n",
      "Epoch 67/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1506 - accuracy: 0.9478 - val_loss: 0.3886 - val_accuracy: 0.9032\n",
      "Epoch 68/100\n",
      "1563/1563 [==============================] - 211s 135ms/step - loss: 0.1460 - accuracy: 0.9498 - val_loss: 0.3230 - val_accuracy: 0.9184\n",
      "Epoch 69/100\n",
      "1563/1563 [==============================] - 211s 135ms/step - loss: 0.1446 - accuracy: 0.9503 - val_loss: 0.3713 - val_accuracy: 0.9118\n",
      "Epoch 70/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1458 - accuracy: 0.9496 - val_loss: 0.3724 - val_accuracy: 0.9101\n",
      "Epoch 71/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1416 - accuracy: 0.9516 - val_loss: 0.3912 - val_accuracy: 0.9108\n",
      "Epoch 72/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1399 - accuracy: 0.9520 - val_loss: 0.3584 - val_accuracy: 0.9085\n",
      "Epoch 73/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1381 - accuracy: 0.9531 - val_loss: 0.4238 - val_accuracy: 0.9024\n",
      "Epoch 74/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1382 - accuracy: 0.9527 - val_loss: 0.3653 - val_accuracy: 0.9107\n",
      "Epoch 75/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1366 - accuracy: 0.9522 - val_loss: 0.3350 - val_accuracy: 0.9153\n",
      "Epoch 76/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1349 - accuracy: 0.9535 - val_loss: 0.4130 - val_accuracy: 0.9089\n",
      "Epoch 77/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1295 - accuracy: 0.9558 - val_loss: 0.4200 - val_accuracy: 0.9071\n",
      "Epoch 78/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1305 - accuracy: 0.9552 - val_loss: 0.3646 - val_accuracy: 0.9122\n",
      "Epoch 79/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1296 - accuracy: 0.9559 - val_loss: 0.3284 - val_accuracy: 0.9201\n",
      "Epoch 80/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1275 - accuracy: 0.9569 - val_loss: 0.3812 - val_accuracy: 0.9131\n",
      "Epoch 81/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1284 - accuracy: 0.9563 - val_loss: 0.3692 - val_accuracy: 0.9110\n",
      "Epoch 82/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1256 - accuracy: 0.9573 - val_loss: 0.3957 - val_accuracy: 0.9066\n",
      "Epoch 83/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1210 - accuracy: 0.9589 - val_loss: 0.3933 - val_accuracy: 0.9136\n",
      "Epoch 84/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1278 - accuracy: 0.9560 - val_loss: 0.3491 - val_accuracy: 0.9151\n",
      "Epoch 85/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1185 - accuracy: 0.9594 - val_loss: 0.3430 - val_accuracy: 0.9188\n",
      "Epoch 86/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1213 - accuracy: 0.9578 - val_loss: 0.3864 - val_accuracy: 0.9127\n",
      "Epoch 87/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1210 - accuracy: 0.9585 - val_loss: 0.4094 - val_accuracy: 0.9073\n",
      "Epoch 88/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1171 - accuracy: 0.9601 - val_loss: 0.3889 - val_accuracy: 0.9124\n",
      "Epoch 89/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1178 - accuracy: 0.9599 - val_loss: 0.3890 - val_accuracy: 0.9135\n",
      "Epoch 90/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1146 - accuracy: 0.9607 - val_loss: 0.3377 - val_accuracy: 0.9186\n",
      "Epoch 91/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1167 - accuracy: 0.9596 - val_loss: 0.3770 - val_accuracy: 0.9121\n",
      "Epoch 92/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1150 - accuracy: 0.9600 - val_loss: 0.3927 - val_accuracy: 0.9146\n",
      "Epoch 93/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1116 - accuracy: 0.9621 - val_loss: 0.3620 - val_accuracy: 0.9158\n",
      "Epoch 94/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1107 - accuracy: 0.9623 - val_loss: 0.3898 - val_accuracy: 0.9101\n",
      "Epoch 95/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1111 - accuracy: 0.9616 - val_loss: 0.3445 - val_accuracy: 0.9204\n",
      "Epoch 96/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1075 - accuracy: 0.9628 - val_loss: 0.4107 - val_accuracy: 0.9082\n",
      "Epoch 97/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1127 - accuracy: 0.9616 - val_loss: 0.3758 - val_accuracy: 0.9163\n",
      "Epoch 98/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1086 - accuracy: 0.9634 - val_loss: 0.4563 - val_accuracy: 0.9009\n",
      "Epoch 99/100\n",
      "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1085 - accuracy: 0.9628 - val_loss: 0.3685 - val_accuracy: 0.9110\n",
      "Epoch 100/100\n",
      "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1080 - accuracy: 0.9627 - val_loss: 0.3701 - val_accuracy: 0.9125\n"
     ]
    }
   ],
   "source": [
    "history_aug = model.fit(\n",
    "    train_gen,\n",
    "    epochs=100,\n",
    "    validation_data=(test_data_proc, test_label_proc),\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b499fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history_aug.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cea65efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7e48f6b91743f3a5be069e8156f3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=4999, description='idx', max=9999), Output()), _dom_classes=('widget-int"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "@interact(idx=(0, 9999, 1))\n",
    "def showTestImage(idx):\n",
    "    data = test_data[idx].reshape(-1, 32, 32, 3)/255.0\n",
    "    dataPred = model.predict(data)\n",
    "    \n",
    "    plt.imshow(test_data[idx].astype('uint16'))\n",
    "    plt.grid(False)\n",
    "    plt.title(f\"LABEL: {class_names[test_label[idx]]}, PREDICT: {class_names[np.argmax(dataPred)]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e44d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "err_idx = []\n",
    "pred_result = model.predict(test_data_proc)\n",
    "\n",
    "for idx in range(10000):\n",
    "    if np.argmax(pred_result[idx]) != test_label[idx]:\n",
    "        err_idx.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "789afd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(875,\n",
       " [3,\n",
       "  33,\n",
       "  37,\n",
       "  47,\n",
       "  58,\n",
       "  59,\n",
       "  74,\n",
       "  86,\n",
       "  87,\n",
       "  118,\n",
       "  127,\n",
       "  128,\n",
       "  147,\n",
       "  158,\n",
       "  164,\n",
       "  165,\n",
       "  178,\n",
       "  192,\n",
       "  195,\n",
       "  210,\n",
       "  213,\n",
       "  226,\n",
       "  228,\n",
       "  237,\n",
       "  255,\n",
       "  264,\n",
       "  273,\n",
       "  287,\n",
       "  293,\n",
       "  313,\n",
       "  315,\n",
       "  324,\n",
       "  352,\n",
       "  355,\n",
       "  356,\n",
       "  378,\n",
       "  384,\n",
       "  394,\n",
       "  396,\n",
       "  405,\n",
       "  422,\n",
       "  426,\n",
       "  433,\n",
       "  449,\n",
       "  456,\n",
       "  480,\n",
       "  483,\n",
       "  485,\n",
       "  488,\n",
       "  526,\n",
       "  537,\n",
       "  598,\n",
       "  640,\n",
       "  665,\n",
       "  672,\n",
       "  683,\n",
       "  689,\n",
       "  725,\n",
       "  727,\n",
       "  731,\n",
       "  734,\n",
       "  768,\n",
       "  770,\n",
       "  776,\n",
       "  779,\n",
       "  798,\n",
       "  810,\n",
       "  811,\n",
       "  831,\n",
       "  835,\n",
       "  866,\n",
       "  885,\n",
       "  888,\n",
       "  916,\n",
       "  924,\n",
       "  925,\n",
       "  929,\n",
       "  943,\n",
       "  956,\n",
       "  966,\n",
       "  972,\n",
       "  982,\n",
       "  989,\n",
       "  991,\n",
       "  1001,\n",
       "  1019,\n",
       "  1026,\n",
       "  1042,\n",
       "  1050,\n",
       "  1051,\n",
       "  1074,\n",
       "  1090,\n",
       "  1093,\n",
       "  1095,\n",
       "  1109,\n",
       "  1120,\n",
       "  1131,\n",
       "  1150,\n",
       "  1163,\n",
       "  1177,\n",
       "  1181,\n",
       "  1201,\n",
       "  1203,\n",
       "  1217,\n",
       "  1222,\n",
       "  1224,\n",
       "  1227,\n",
       "  1247,\n",
       "  1263,\n",
       "  1277,\n",
       "  1280,\n",
       "  1299,\n",
       "  1321,\n",
       "  1322,\n",
       "  1323,\n",
       "  1325,\n",
       "  1339,\n",
       "  1353,\n",
       "  1355,\n",
       "  1393,\n",
       "  1398,\n",
       "  1429,\n",
       "  1439,\n",
       "  1446,\n",
       "  1453,\n",
       "  1461,\n",
       "  1470,\n",
       "  1474,\n",
       "  1479,\n",
       "  1492,\n",
       "  1495,\n",
       "  1496,\n",
       "  1506,\n",
       "  1507,\n",
       "  1529,\n",
       "  1530,\n",
       "  1536,\n",
       "  1552,\n",
       "  1574,\n",
       "  1579,\n",
       "  1581,\n",
       "  1595,\n",
       "  1640,\n",
       "  1644,\n",
       "  1649,\n",
       "  1684,\n",
       "  1685,\n",
       "  1698,\n",
       "  1715,\n",
       "  1718,\n",
       "  1721,\n",
       "  1727,\n",
       "  1731,\n",
       "  1732,\n",
       "  1739,\n",
       "  1746,\n",
       "  1759,\n",
       "  1765,\n",
       "  1779,\n",
       "  1788,\n",
       "  1790,\n",
       "  1796,\n",
       "  1804,\n",
       "  1811,\n",
       "  1813,\n",
       "  1823,\n",
       "  1835,\n",
       "  1837,\n",
       "  1858,\n",
       "  1884,\n",
       "  1885,\n",
       "  1895,\n",
       "  1898,\n",
       "  1909,\n",
       "  1912,\n",
       "  1935,\n",
       "  1939,\n",
       "  1940,\n",
       "  1950,\n",
       "  1956,\n",
       "  1957,\n",
       "  1959,\n",
       "  1969,\n",
       "  2002,\n",
       "  2010,\n",
       "  2033,\n",
       "  2046,\n",
       "  2058,\n",
       "  2059,\n",
       "  2061,\n",
       "  2065,\n",
       "  2078,\n",
       "  2085,\n",
       "  2087,\n",
       "  2088,\n",
       "  2091,\n",
       "  2096,\n",
       "  2108,\n",
       "  2111,\n",
       "  2127,\n",
       "  2128,\n",
       "  2130,\n",
       "  2154,\n",
       "  2157,\n",
       "  2165,\n",
       "  2166,\n",
       "  2171,\n",
       "  2172,\n",
       "  2174,\n",
       "  2175,\n",
       "  2186,\n",
       "  2187,\n",
       "  2216,\n",
       "  2226,\n",
       "  2232,\n",
       "  2242,\n",
       "  2248,\n",
       "  2251,\n",
       "  2262,\n",
       "  2270,\n",
       "  2271,\n",
       "  2273,\n",
       "  2292,\n",
       "  2293,\n",
       "  2298,\n",
       "  2301,\n",
       "  2305,\n",
       "  2331,\n",
       "  2352,\n",
       "  2355,\n",
       "  2362,\n",
       "  2381,\n",
       "  2405,\n",
       "  2433,\n",
       "  2455,\n",
       "  2472,\n",
       "  2473,\n",
       "  2494,\n",
       "  2495,\n",
       "  2511,\n",
       "  2525,\n",
       "  2530,\n",
       "  2532,\n",
       "  2548,\n",
       "  2588,\n",
       "  2590,\n",
       "  2592,\n",
       "  2597,\n",
       "  2599,\n",
       "  2629,\n",
       "  2650,\n",
       "  2675,\n",
       "  2688,\n",
       "  2722,\n",
       "  2746,\n",
       "  2754,\n",
       "  2770,\n",
       "  2777,\n",
       "  2779,\n",
       "  2783,\n",
       "  2798,\n",
       "  2804,\n",
       "  2825,\n",
       "  2831,\n",
       "  2835,\n",
       "  2843,\n",
       "  2845,\n",
       "  2854,\n",
       "  2855,\n",
       "  2889,\n",
       "  2895,\n",
       "  2912,\n",
       "  2948,\n",
       "  2955,\n",
       "  2958,\n",
       "  2959,\n",
       "  2968,\n",
       "  2981,\n",
       "  3016,\n",
       "  3025,\n",
       "  3031,\n",
       "  3032,\n",
       "  3052,\n",
       "  3070,\n",
       "  3072,\n",
       "  3082,\n",
       "  3084,\n",
       "  3113,\n",
       "  3130,\n",
       "  3164,\n",
       "  3168,\n",
       "  3171,\n",
       "  3180,\n",
       "  3192,\n",
       "  3194,\n",
       "  3202,\n",
       "  3207,\n",
       "  3211,\n",
       "  3228,\n",
       "  3236,\n",
       "  3251,\n",
       "  3278,\n",
       "  3283,\n",
       "  3297,\n",
       "  3299,\n",
       "  3306,\n",
       "  3309,\n",
       "  3320,\n",
       "  3335,\n",
       "  3336,\n",
       "  3342,\n",
       "  3343,\n",
       "  3346,\n",
       "  3349,\n",
       "  3352,\n",
       "  3354,\n",
       "  3359,\n",
       "  3382,\n",
       "  3387,\n",
       "  3416,\n",
       "  3422,\n",
       "  3434,\n",
       "  3451,\n",
       "  3452,\n",
       "  3472,\n",
       "  3489,\n",
       "  3490,\n",
       "  3494,\n",
       "  3497,\n",
       "  3514,\n",
       "  3522,\n",
       "  3550,\n",
       "  3560,\n",
       "  3571,\n",
       "  3574,\n",
       "  3587,\n",
       "  3594,\n",
       "  3596,\n",
       "  3600,\n",
       "  3607,\n",
       "  3615,\n",
       "  3622,\n",
       "  3625,\n",
       "  3631,\n",
       "  3646,\n",
       "  3653,\n",
       "  3658,\n",
       "  3680,\n",
       "  3696,\n",
       "  3704,\n",
       "  3708,\n",
       "  3716,\n",
       "  3735,\n",
       "  3752,\n",
       "  3753,\n",
       "  3755,\n",
       "  3761,\n",
       "  3779,\n",
       "  3787,\n",
       "  3789,\n",
       "  3799,\n",
       "  3808,\n",
       "  3828,\n",
       "  3867,\n",
       "  3890,\n",
       "  3892,\n",
       "  3897,\n",
       "  3909,\n",
       "  3920,\n",
       "  3949,\n",
       "  3956,\n",
       "  3957,\n",
       "  3977,\n",
       "  3980,\n",
       "  3995,\n",
       "  4001,\n",
       "  4012,\n",
       "  4013,\n",
       "  4020,\n",
       "  4038,\n",
       "  4051,\n",
       "  4052,\n",
       "  4055,\n",
       "  4056,\n",
       "  4066,\n",
       "  4071,\n",
       "  4097,\n",
       "  4106,\n",
       "  4139,\n",
       "  4140,\n",
       "  4152,\n",
       "  4162,\n",
       "  4163,\n",
       "  4169,\n",
       "  4174,\n",
       "  4192,\n",
       "  4198,\n",
       "  4204,\n",
       "  4210,\n",
       "  4276,\n",
       "  4302,\n",
       "  4309,\n",
       "  4368,\n",
       "  4373,\n",
       "  4404,\n",
       "  4457,\n",
       "  4458,\n",
       "  4463,\n",
       "  4479,\n",
       "  4481,\n",
       "  4485,\n",
       "  4487,\n",
       "  4490,\n",
       "  4528,\n",
       "  4546,\n",
       "  4552,\n",
       "  4555,\n",
       "  4561,\n",
       "  4565,\n",
       "  4590,\n",
       "  4593,\n",
       "  4606,\n",
       "  4614,\n",
       "  4615,\n",
       "  4618,\n",
       "  4624,\n",
       "  4630,\n",
       "  4632,\n",
       "  4646,\n",
       "  4686,\n",
       "  4696,\n",
       "  4703,\n",
       "  4705,\n",
       "  4710,\n",
       "  4740,\n",
       "  4744,\n",
       "  4748,\n",
       "  4749,\n",
       "  4750,\n",
       "  4754,\n",
       "  4775,\n",
       "  4776,\n",
       "  4784,\n",
       "  4794,\n",
       "  4795,\n",
       "  4829,\n",
       "  4849,\n",
       "  4871,\n",
       "  4892,\n",
       "  4901,\n",
       "  4906,\n",
       "  4910,\n",
       "  4921,\n",
       "  4924,\n",
       "  4931,\n",
       "  4938,\n",
       "  4942,\n",
       "  4947,\n",
       "  4951,\n",
       "  4964,\n",
       "  4965,\n",
       "  4966,\n",
       "  4987,\n",
       "  4995,\n",
       "  4996,\n",
       "  4998,\n",
       "  5011,\n",
       "  5030,\n",
       "  5040,\n",
       "  5046,\n",
       "  5053,\n",
       "  5056,\n",
       "  5072,\n",
       "  5074,\n",
       "  5077,\n",
       "  5101,\n",
       "  5102,\n",
       "  5112,\n",
       "  5130,\n",
       "  5141,\n",
       "  5151,\n",
       "  5154,\n",
       "  5166,\n",
       "  5172,\n",
       "  5177,\n",
       "  5192,\n",
       "  5193,\n",
       "  5200,\n",
       "  5211,\n",
       "  5213,\n",
       "  5218,\n",
       "  5224,\n",
       "  5272,\n",
       "  5273,\n",
       "  5281,\n",
       "  5287,\n",
       "  5290,\n",
       "  5335,\n",
       "  5352,\n",
       "  5354,\n",
       "  5367,\n",
       "  5377,\n",
       "  5387,\n",
       "  5416,\n",
       "  5417,\n",
       "  5424,\n",
       "  5426,\n",
       "  5455,\n",
       "  5458,\n",
       "  5468,\n",
       "  5498,\n",
       "  5511,\n",
       "  5529,\n",
       "  5534,\n",
       "  5537,\n",
       "  5548,\n",
       "  5582,\n",
       "  5609,\n",
       "  5632,\n",
       "  5634,\n",
       "  5642,\n",
       "  5659,\n",
       "  5679,\n",
       "  5690,\n",
       "  5708,\n",
       "  5730,\n",
       "  5732,\n",
       "  5734,\n",
       "  5736,\n",
       "  5774,\n",
       "  5797,\n",
       "  5806,\n",
       "  5819,\n",
       "  5830,\n",
       "  5835,\n",
       "  5862,\n",
       "  5864,\n",
       "  5872,\n",
       "  5878,\n",
       "  5903,\n",
       "  5910,\n",
       "  5917,\n",
       "  5940,\n",
       "  5959,\n",
       "  5972,\n",
       "  5979,\n",
       "  5981,\n",
       "  6008,\n",
       "  6014,\n",
       "  6016,\n",
       "  6023,\n",
       "  6025,\n",
       "  6031,\n",
       "  6068,\n",
       "  6077,\n",
       "  6101,\n",
       "  6135,\n",
       "  6142,\n",
       "  6143,\n",
       "  6146,\n",
       "  6151,\n",
       "  6154,\n",
       "  6162,\n",
       "  6163,\n",
       "  6178,\n",
       "  6197,\n",
       "  6202,\n",
       "  6218,\n",
       "  6228,\n",
       "  6245,\n",
       "  6251,\n",
       "  6253,\n",
       "  6257,\n",
       "  6326,\n",
       "  6339,\n",
       "  6353,\n",
       "  6359,\n",
       "  6393,\n",
       "  6412,\n",
       "  6419,\n",
       "  6428,\n",
       "  6434,\n",
       "  6480,\n",
       "  6499,\n",
       "  6522,\n",
       "  6533,\n",
       "  6541,\n",
       "  6544,\n",
       "  6545,\n",
       "  6574,\n",
       "  6588,\n",
       "  6590,\n",
       "  6640,\n",
       "  6646,\n",
       "  6680,\n",
       "  6683,\n",
       "  6726,\n",
       "  6729,\n",
       "  6748,\n",
       "  6750,\n",
       "  6753,\n",
       "  6764,\n",
       "  6774,\n",
       "  6781,\n",
       "  6783,\n",
       "  6786,\n",
       "  6792,\n",
       "  6799,\n",
       "  6833,\n",
       "  6845,\n",
       "  6851,\n",
       "  6853,\n",
       "  6859,\n",
       "  6865,\n",
       "  6912,\n",
       "  6962,\n",
       "  6966,\n",
       "  6969,\n",
       "  6970,\n",
       "  6973,\n",
       "  6979,\n",
       "  6985,\n",
       "  6994,\n",
       "  7000,\n",
       "  7002,\n",
       "  7004,\n",
       "  7005,\n",
       "  7040,\n",
       "  7099,\n",
       "  7107,\n",
       "  7112,\n",
       "  7119,\n",
       "  7168,\n",
       "  7202,\n",
       "  7218,\n",
       "  7232,\n",
       "  7246,\n",
       "  7257,\n",
       "  7311,\n",
       "  7346,\n",
       "  7360,\n",
       "  7366,\n",
       "  7367,\n",
       "  7384,\n",
       "  7398,\n",
       "  7399,\n",
       "  7402,\n",
       "  7406,\n",
       "  7408,\n",
       "  7411,\n",
       "  7416,\n",
       "  7420,\n",
       "  7436,\n",
       "  7444,\n",
       "  7446,\n",
       "  7469,\n",
       "  7482,\n",
       "  7491,\n",
       "  7496,\n",
       "  7509,\n",
       "  7556,\n",
       "  7559,\n",
       "  7561,\n",
       "  7590,\n",
       "  7600,\n",
       "  7602,\n",
       "  7605,\n",
       "  7609,\n",
       "  7610,\n",
       "  7625,\n",
       "  7644,\n",
       "  7648,\n",
       "  7657,\n",
       "  7661,\n",
       "  7665,\n",
       "  7670,\n",
       "  7685,\n",
       "  7730,\n",
       "  7744,\n",
       "  7761,\n",
       "  7778,\n",
       "  7785,\n",
       "  7792,\n",
       "  7813,\n",
       "  7815,\n",
       "  7821,\n",
       "  7824,\n",
       "  7827,\n",
       "  7842,\n",
       "  7853,\n",
       "  7858,\n",
       "  7875,\n",
       "  7881,\n",
       "  7892,\n",
       "  7909,\n",
       "  7918,\n",
       "  7921,\n",
       "  7926,\n",
       "  7931,\n",
       "  7940,\n",
       "  7941,\n",
       "  7967,\n",
       "  7976,\n",
       "  7989,\n",
       "  8014,\n",
       "  8033,\n",
       "  8066,\n",
       "  8104,\n",
       "  8110,\n",
       "  8115,\n",
       "  8118,\n",
       "  8166,\n",
       "  8169,\n",
       "  8173,\n",
       "  8185,\n",
       "  8197,\n",
       "  8198,\n",
       "  8236,\n",
       "  8268,\n",
       "  8269,\n",
       "  8281,\n",
       "  8296,\n",
       "  8316,\n",
       "  8324,\n",
       "  8340,\n",
       "  8363,\n",
       "  8394,\n",
       "  8419,\n",
       "  8420,\n",
       "  8428,\n",
       "  8432,\n",
       "  8440,\n",
       "  8446,\n",
       "  8450,\n",
       "  8451,\n",
       "  8457,\n",
       "  8469,\n",
       "  8480,\n",
       "  8484,\n",
       "  8488,\n",
       "  8490,\n",
       "  8492,\n",
       "  8511,\n",
       "  8514,\n",
       "  8521,\n",
       "  8535,\n",
       "  8542,\n",
       "  8546,\n",
       "  8548,\n",
       "  8557,\n",
       "  8576,\n",
       "  8577,\n",
       "  8578,\n",
       "  8580,\n",
       "  8589,\n",
       "  8618,\n",
       "  8625,\n",
       "  8633,\n",
       "  8642,\n",
       "  8650,\n",
       "  8671,\n",
       "  8677,\n",
       "  8699,\n",
       "  8720,\n",
       "  8723,\n",
       "  8728,\n",
       "  8743,\n",
       "  8749,\n",
       "  8752,\n",
       "  8779,\n",
       "  8782,\n",
       "  8794,\n",
       "  8803,\n",
       "  8809,\n",
       "  8811,\n",
       "  8827,\n",
       "  8855,\n",
       "  8869,\n",
       "  8872,\n",
       "  8886,\n",
       "  8925,\n",
       "  8938,\n",
       "  8941,\n",
       "  8977,\n",
       "  8981,\n",
       "  8983,\n",
       "  8993,\n",
       "  9011,\n",
       "  9031,\n",
       "  9039,\n",
       "  9041,\n",
       "  9065,\n",
       "  9073,\n",
       "  9085,\n",
       "  9107,\n",
       "  9132,\n",
       "  9143,\n",
       "  9160,\n",
       "  9170,\n",
       "  9216,\n",
       "  9227,\n",
       "  9230,\n",
       "  9234,\n",
       "  9235,\n",
       "  9237,\n",
       "  9248,\n",
       "  9254,\n",
       "  9260,\n",
       "  9269,\n",
       "  9292,\n",
       "  9303,\n",
       "  9341,\n",
       "  9346,\n",
       "  9369,\n",
       "  9375,\n",
       "  9385,\n",
       "  9414,\n",
       "  9419,\n",
       "  9421,\n",
       "  9422,\n",
       "  9431,\n",
       "  9435,\n",
       "  9465,\n",
       "  9467,\n",
       "  9503,\n",
       "  9506,\n",
       "  9507,\n",
       "  9518,\n",
       "  9522,\n",
       "  9528,\n",
       "  9535,\n",
       "  9555,\n",
       "  9562,\n",
       "  9573,\n",
       "  9575,\n",
       "  9587,\n",
       "  9609,\n",
       "  9613,\n",
       "  9616,\n",
       "  9622,\n",
       "  9633,\n",
       "  9643,\n",
       "  9652,\n",
       "  9689,\n",
       "  9704,\n",
       "  9707,\n",
       "  9740,\n",
       "  9746,\n",
       "  9764,\n",
       "  9776,\n",
       "  9779,\n",
       "  9793,\n",
       "  9801,\n",
       "  9812,\n",
       "  9819,\n",
       "  9832,\n",
       "  9840,\n",
       "  9842,\n",
       "  9847,\n",
       "  9853,\n",
       "  9857,\n",
       "  9873,\n",
       "  9880,\n",
       "  9884,\n",
       "  9901,\n",
       "  9918,\n",
       "  9949,\n",
       "  9953,\n",
       "  9959,\n",
       "  9960,\n",
       "  9967,\n",
       "  9982,\n",
       "  9985,\n",
       "  9989,\n",
       "  9996])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(err_idx), err_idx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
